{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core Python Libraries\n",
    "import numpy as np          \n",
    "import pandas as pd            \n",
    "\n",
    "# Statistics & Scientific Computing\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    "    StandardScaler\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Machine Learning - Model Selection & Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Advanced Models\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Handling Class Imbalance\n",
    "from imblearn.over_sampling import SMOTE, SMOTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e6aaf",
   "metadata": {},
   "source": [
    "# MODELING\n",
    "\n",
    "With exploratory data analysis complete and a clear understanding of the dataset, we now transition into building predictive models.  \n",
    "Our target variable is **status_group**, which categorizes water points as *functional*, *functional needs repair*, or *non functional*.  \n",
    "The predictor variables used for modeling are:\n",
    "\n",
    "- **gps_height**  \n",
    "- **installer**  \n",
    "- **longitude, latitude**  \n",
    "- **basin**  \n",
    "- **region**  \n",
    "- **population**  \n",
    "- **permit**  \n",
    "- **construction_year**  \n",
    "- **extraction_type_group**  \n",
    "- **management_group**  \n",
    "- **payment_type**  \n",
    "- **water_quality**  \n",
    "- **quantity**  \n",
    "- **source**  \n",
    "- **waterpoint_type_group**\n",
    "\n",
    "## Preprocessing\n",
    "Before modeling, we will:\n",
    "- Handle missing values and ensure data completeness.  \n",
    "- Encode categorical features (e.g installer, basin, region).  \n",
    "- Scale numerical variables where appropriate (e.g gps_height, population).  \n",
    "- Address class imbalance in status_group using techniques such as **SMOTE/SMOTEN**.  \n",
    "\n",
    "## Baseline Model\n",
    "We begin with a **Dummy Classifier** to establish a baseline accuracy. This provides a benchmark to measure the value added by more sophisticated models.\n",
    "\n",
    "## Candidate Models\n",
    "We then train and evaluate several classifiers:  \n",
    "- **Logistic Regression** – interpretable baseline linear model.  \n",
    "- **Decision Tree Classifier** – captures nonlinear relationships and feature interactions.  \n",
    "- **Random Forest Classifier** – ensemble approach to reduce variance and improve stability.  \n",
    "- **K-Nearest Neighbors (KNN)** – instance-based learning for comparison.  \n",
    "- **XGBoost Classifier** – gradient boosting ensemble, often highly effective in structured data.  \n",
    "\n",
    "## Model Evaluation\n",
    "Models will be compared on:  \n",
    "- **Accuracy** – overall correctness.  \n",
    "- **Precision, Recall, F1-score** – to account for imbalanced classes.  \n",
    "- **Confusion Matrix** – to visualize class-level performance.  \n",
    "- **ROC Curve and AUC** – to assess discriminative power.  \n",
    "\n",
    "## Model Selection\n",
    "Based on evaluation metrics, we will select the best-performing model. The winning model should not only achieve high predictive performance but also generalize well to unseen data. If multiple models perform competitively, additional considerations such as interpretability and computational efficiency will guide the final choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006f5e1",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98c51c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59400, 41)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training values (features)\n",
    "train_values_df = pd.read_csv(\"C:\\\\Users\\\\lenovo\\\\OneDrive\\\\Desktop\\\\DS\\\\PROJECTS\\\\tanzania-water-well-status-prediction\\\\Data\\\\Training set values.csv\")\n",
    "\n",
    "# Training labels (targets)\n",
    "train_labels_df = pd.read_csv(\"C:\\\\Users\\\\lenovo\\\\OneDrive\\\\Desktop\\\\DS\\\\PROJECTS\\\\tanzania-water-well-status-prediction\\\\Data\\\\Training set labels.csv\")\n",
    "\n",
    "# Load training data\n",
    "train_df = train_values_df.merge(train_labels_df, on = 'id')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da5d901",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.13 MiB for an array with shape (20, 59400) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# One-hot encode categorical features\u001b[39;00m\n\u001b[32m     17\u001b[39m cat_cols = X.select_dtypes(include=[\u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m]).columns\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 3. Scale numeric variables\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\anaconda3\\envs\\ds-env\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:224\u001b[39m, in \u001b[36mget_dummies\u001b[39m\u001b[34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[39m\n\u001b[32m    214\u001b[39m         dummy = _get_dummies_1d(\n\u001b[32m    215\u001b[39m             col[\u001b[32m1\u001b[39m],\n\u001b[32m    216\u001b[39m             prefix=pre,\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m             dtype=dtype,\n\u001b[32m    222\u001b[39m         )\n\u001b[32m    223\u001b[39m         with_dummies.append(dummy)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     result = concat(with_dummies, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     result = _get_dummies_1d(\n\u001b[32m    227\u001b[39m         data,\n\u001b[32m    228\u001b[39m         prefix,\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m         dtype=dtype,\n\u001b[32m    234\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\anaconda3\\envs\\ds-env\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\anaconda3\\envs\\ds-env\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = concatenate_managers(\n\u001b[32m    685\u001b[39m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m.new_axes, concat_axis=\u001b[38;5;28mself\u001b[39m.bm_axis, copy=\u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m    686\u001b[39m )\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\anaconda3\\envs\\ds-env\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m concat_axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     mgrs = _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[32m0\u001b[39m].concat_horizontal(mgrs, axes)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].nblocks > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\anaconda3\\envs\\ds-env\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[39m, in \u001b[36m_maybe_reindex_columns_na_proxy\u001b[39m\u001b[34m(axes, mgrs_indexers, needs_copy)\u001b[39m\n\u001b[32m    220\u001b[39m         mgr = mgr.reindex_indexer(\n\u001b[32m    221\u001b[39m             axes[i],\n\u001b[32m    222\u001b[39m             indexers[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m             use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[32m    228\u001b[39m         )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         mgr = mgr.copy()\n\u001b[32m    232\u001b[39m     new_mgrs.append(mgr)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\anaconda3\\envs\\ds-env\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    591\u001b[39m         new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m res = \u001b[38;5;28mself\u001b[39m.apply(\u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m, deep=deep)\n\u001b[32m    594\u001b[39m res.axes = new_axes\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    597\u001b[39m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\anaconda3\\envs\\ds-env\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28mgetattr\u001b[39m(b, f)(**kwargs)\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\anaconda3\\envs\\ds-env\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:796\u001b[39m, in \u001b[36mBlock.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    794\u001b[39m refs: BlockValuesRefs | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m     values = values.copy()\n\u001b[32m    797\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.13 MiB for an array with shape (20, 59400) and data type bool"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Separate features and target\n",
    "# ===============================\n",
    "X = train_df.drop('status_group', axis=1)\n",
    "y = train_df['status_group']\n",
    "\n",
    "# ===============================\n",
    "# 2. Encode categorical variables\n",
    "# ===============================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# ===============================\n",
    "# 3. Scale numeric variables\n",
    "# ===============================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = ['gps_height', 'longitude', 'latitude', 'population', 'construction_year']\n",
    "scaler = StandardScaler()\n",
    "X_encoded[num_cols] = scaler.fit_transform(X_encoded[num_cols])\n",
    "\n",
    "# ===============================\n",
    "# 4. Train-test split\n",
    "# ===============================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_encoded, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 5. Handle class imbalance\n",
    "# ===============================\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Original training set shape:\", X_train.shape)\n",
    "print(\"Resampled training set shape:\", X_train_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3eda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
